{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Project Description`\n",
    "Imagine being at the helm of a business with a treasure trove of transactional data, brimming with untapped potential, yet unable to harness its power. That's where our client finds themselvesâ€”a company sitting on a goldmine of 2019 transactional data, eager to uncover insights that could propel their business to new heights. They've turned to us, seeking a transformative business intelligence solution that not only answers their pressing questions but also illuminates hidden opportunities to boost sales and streamline operations.\n",
    "\n",
    "Enter getINNOtized, a dynamic organization dedicated to connecting talented data professionals with businesses in need of innovative data solutions. Through their platform, companies can leverage top-tier analytical expertise to unlock the full potential of their data. getINNOtized has assigned us this mission, confident in our ability to deliver a comprehensive, actionable business intelligence report that will empower the client to make informed, strategic decisions. Our task is clear: dive deep into this data, decode its secrets, and deliver insights that guide the client towards increased revenue and enhanced efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Key Stakeholders**\n",
    "The stakeholders include the client's company executives, sales and marketing team, product management team, logistics and supply chain team, IT and data team, getINNOtized and external stakeholders(investors and suppliers)\n",
    "\n",
    "#### **Success Criteria**\n",
    "\n",
    "The success of this project will be measured by the ability to deliver a detailed and actionable business intelligence report that answers the client's questions and provides insights for driving sales and efficiency. The report should be clear, visually appealing, and easy to understand, providing the client with a solid foundation for making data-driven decisions.\n",
    "\n",
    "#### **Constraints and Considerations**\n",
    "\n",
    "- Data Quality: Ensure the data is clean, accurate, and complete before analysis.\n",
    "- Timeliness: The analysis and report should be delivered within the agreed-upon timeframe.\n",
    "- Client Collaboration: Regular communication with the client to understand their needs and provide updates on progress.\n",
    "- Tool Selection: Utilize appropriate data analysis and visualization tools to generate insights and present findings effectively.\n",
    "\n",
    "#### **Data Requirements**\n",
    "- Utilize data that was collected for each month in the entire year of 2019. The data for the first half of the year (January to June) was collected in excel and saved as csv files before management decided to use databases to store their data for analysis.\n",
    "\n",
    "**<i>NB</i>** Additionally, categorize products based on their unit prices:\n",
    "- Products with unit prices above $99.99 should be labeled as high-level products.\n",
    "- Products with unit prices $99.99 and below should be labeled as basic-level products.\n",
    "\n",
    "#### **Business Impact**\n",
    "- Enhance customer satisfaction through better product availability.\n",
    "- Optimize inventory management, leading to cost savings and improved operational efficiency.\n",
    "\n",
    "### `Hypothesis`\n",
    "\n",
    "*Null Hypothesis (Ho):* \n",
    "\n",
    "*Alternate Hypothesis (Ha):* \n",
    "\n",
    "### `Analytical Business Questions`\n",
    "\n",
    "1. How much money did we make this year?\n",
    "2. Can we identify any seasonality in the  sales?\n",
    "3. What are our best and worst-selling products?\n",
    "4. How do sales compare to previous months or weeks? \n",
    "5. Which cities are our products delivered to most? \n",
    "6. How do product categories compare in revenue generated and quantities  ordered?\n",
    "7. You are required to show additional details from your findings in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Importations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PACKAGE SUCCESS!! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# Data Connection\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Stats Packages\n",
    "from scipy.stats import stats\n",
    "\n",
    "\n",
    "# Others\n",
    "import os\n",
    "from itertools import product\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"PACKAGE SUCCESS!! ðŸŽ‰\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Data Connection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables=dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the '.env' file\n",
    "server = environment_variables.get(\"server_name\")\n",
    "database = environment_variables.get(\"database_name\")\n",
    "username = environment_variables.get(\"user\")\n",
    "password = environment_variables.get(\"password\")\n",
    "\n",
    "connection_string=f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "\n",
    "# Use the connect method of the pyodbc library and pass in the connection string.\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales_August_2019\n",
      "Sales_December_2019\n",
      "Sales_July_2019\n",
      "Sales_November_2019\n",
      "Sales_October_2019\n",
      "Sales_September_2019\n",
      "change_streams_destination_type\n",
      "change_streams_partition_scheme\n",
      "trace_xe_action_map\n",
      "trace_xe_event_map\n"
     ]
    }
   ],
   "source": [
    "# Get the cursor\n",
    "# The connection cursor is used to execute statements to communicate with the MySQL database\n",
    "cursor = connection.cursor()\n",
    "# Retrieve the table names\n",
    "table_names = cursor.tables(tableType='TABLE')\n",
    "# Fetch all the table names\n",
    "tables = table_names.fetchall()\n",
    "# Print the table names\n",
    "for table in tables:\n",
    "    print(table.table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query to get the datasets\n",
    "query = \"SELECT * FROM Sales_August_2019\"\n",
    "query2 = \"SELECT * FROM Sales_December_2019\"\n",
    "query3 = \"SELECT * FROM Sales_July_2019\"\n",
    "query4 = \"SELECT * FROM Sales_November_2019\"\n",
    "query5 = \"SELECT * FROM Sales_October_2019\"\n",
    "query6 = \"SELECT * FROM Sales_September_2019\"\n",
    "\n",
    "data=pd.read_sql(query,connection)\n",
    "data2=pd.read_sql(query2,connection)\n",
    "data3=pd.read_sql(query3,connection)\n",
    "data4=pd.read_sql(query4,connection)\n",
    "data5=pd.read_sql(query5,connection)\n",
    "data6=pd.read_sql(query6,connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tables to csv\n",
    "data.to_csv('data\\Sales_August_2019.csv', index=False)\n",
    "data2.to_csv('data\\Sales_December_2019.csv', index=False)\n",
    "data3.to_csv('data\\Sales_July_2019.csv', index=False)\n",
    "data4.to_csv('data\\Sales_November_2019.csv', index=False)\n",
    "data5.to_csv('data\\Sales_October_2019.csv', index=False)\n",
    "data6.to_csv('data\\Sales_September_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14371 entries, 0 to 14370\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Order_ID          14291 non-null  float64\n",
      " 1   Product           14326 non-null  object \n",
      " 2   Quantity_Ordered  14291 non-null  float64\n",
      " 3   Price_Each        14291 non-null  float64\n",
      " 4   Order_Date        14291 non-null  object \n",
      " 5   Purchase_Address  14326 non-null  object \n",
      "dtypes: float64(3), object(3)\n",
      "memory usage: 673.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV and change date column from object to date type\n",
    "sales_Aug = pd.read_csv(\"data\\Sales_August_2019.csv\", parse_dates =['Order_Date'])\n",
    "sales_Dec = pd.read_csv(\"data\\Sales_December_2019.csv\", parse_dates =['Order_Date'])\n",
    "sales_Jul = pd.read_csv(\"data\\Sales_July_2019.csv\",parse_dates=['Order_Date'])\n",
    "sales_Nov = pd.read_csv(\"data\\Sales_November_2019.csv\",parse_dates=['Order_Date'])\n",
    "sales_Oct = pd.read_csv(\"data\\Sales_October_2019.csv\",parse_dates=['Order_Date'])\n",
    "sales_Sept = pd.read_csv(\"data\\Sales_September_2019.csv\",parse_dates=['Order_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17661 entries, 0 to 17660\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   Order_ID          17580 non-null  float64       \n",
      " 1   Product           17616 non-null  object        \n",
      " 2   Quantity_Ordered  17580 non-null  float64       \n",
      " 3   Price_Each        17580 non-null  float64       \n",
      " 4   Order_Date        17580 non-null  datetime64[ns]\n",
      " 5   Purchase_Address  17616 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(2)\n",
      "memory usage: 828.0+ KB\n"
     ]
    }
   ],
   "source": [
    "sales_Nov.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading First 6 months of the data can be found in this OneDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data7 = pd.read_excel('C:\\\\Users\\\\KEMUNTO\\\\Downloads\\\\Sales_April_2019.xlsx')\n",
    "data8 = pd.read_excel('C:\\\\Users\\\\KEMUNTO\\\\Downloads\\\\Sales_February_2019.xlsx')\n",
    "data9 = pd.read_excel('C:\\\\Users\\\\KEMUNTO\\\\Downloads\\\\Sales_January_2019.xlsx')\n",
    "data10 = pd.read_excel('C:\\\\Users\\\\KEMUNTO\\\\Downloads\\\\Sales_June_2019.xlsx')\n",
    "data11 = pd.read_excel('C:\\\\Users\\\\KEMUNTO\\\\Downloads\\\\Sales_March_2019.xlsx')\n",
    "data12 = pd.read_excel('C:\\\\Users\\\\KEMUNTO\\\\Downloads\\\\Sales_May_2019.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13622 entries, 0 to 13621\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Order ID          13579 non-null  object\n",
      " 1   Product           13579 non-null  object\n",
      " 2   Quantity Ordered  13579 non-null  object\n",
      " 3   Price Each        13579 non-null  object\n",
      " 4   Order Date        13579 non-null  object\n",
      " 5   Purchase Address  13579 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 638.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data10.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>209921</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>2019-06-23 19:34:00</td>\n",
       "      <td>950 Walnut St, Portland, ME 04101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209922</td>\n",
       "      <td>Macbook Pro Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2019-06-30 10:05:00</td>\n",
       "      <td>80 4th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209923</td>\n",
       "      <td>ThinkPad Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>999.99</td>\n",
       "      <td>2019-06-24 20:18:00</td>\n",
       "      <td>402 Jackson St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>209924</td>\n",
       "      <td>27in FHD Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>149.99</td>\n",
       "      <td>2019-06-05 10:21:00</td>\n",
       "      <td>560 10th St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>209925</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>2019-06-25 18:58:00</td>\n",
       "      <td>545 2nd St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0   209921        USB-C Charging Cable                1      11.95   \n",
       "1   209922          Macbook Pro Laptop                1       1700   \n",
       "2   209923             ThinkPad Laptop                1     999.99   \n",
       "3   209924            27in FHD Monitor                1     149.99   \n",
       "4   209925  Bose SoundSport Headphones                1      99.99   \n",
       "\n",
       "            Order Date                       Purchase Address  \n",
       "0  2019-06-23 19:34:00      950 Walnut St, Portland, ME 04101  \n",
       "1  2019-06-30 10:05:00     80 4th St, San Francisco, CA 94016  \n",
       "2  2019-06-24 20:18:00  402 Jackson St, Los Angeles, CA 90001  \n",
       "3  2019-06-05 10:21:00         560 10th St, Seattle, WA 98101  \n",
       "4  2019-06-25 18:58:00    545 2nd St, San Francisco, CA 94016  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tables to csv\n",
    "data7.to_csv('data\\Sales_April_2019.csv', index=False)\n",
    "data8.to_csv('data\\Sales_February_2019.csv', index=False)\n",
    "data9.to_csv('data\\Sales_January_2019.csv', index=False)\n",
    "data10.to_csv('data\\Sales_June_2019.csv', index=False)\n",
    "data11.to_csv('data\\Sales_March_2019.csv', index=False)\n",
    "data12.to_csv('data\\Sales_May_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing column provided to 'parse_dates': 'Order_Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read CSV and change date column from object to date type\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sales_Apr \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSales_April_2019.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOrder_Date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m sales_Feb \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSales_February_2019.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, parse_dates \u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrder_Date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m sales_Jan \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSales_January_2019.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrder_Date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\KEMUNTO\\Desktop\\Capstone\\A-Power-BI-Dashboard-Project\\virtual_capstone\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KEMUNTO\\Desktop\\Capstone\\A-Power-BI-Dashboard-Project\\virtual_capstone\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\KEMUNTO\\Desktop\\Capstone\\A-Power-BI-Dashboard-Project\\virtual_capstone\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KEMUNTO\\Desktop\\Capstone\\A-Power-BI-Dashboard-Project\\virtual_capstone\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\KEMUNTO\\Desktop\\Capstone\\A-Power-BI-Dashboard-Project\\virtual_capstone\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:161\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_usecols_names(\n\u001b[0;32m    156\u001b[0m             usecols,\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    158\u001b[0m         )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_parse_dates_presence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_noconvert_columns()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KEMUNTO\\Desktop\\Capstone\\A-Power-BI-Dashboard-Project\\virtual_capstone\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:243\u001b[0m, in \u001b[0;36mParserBase._validate_parse_dates_presence\u001b[1;34m(self, columns)\u001b[0m\n\u001b[0;32m    233\u001b[0m missing_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m    235\u001b[0m         {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing column provided to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_dates\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Convert positions to actual column names\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    248\u001b[0m     col \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns) \u001b[38;5;28;01melse\u001b[39;00m columns[col]\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_needed\n\u001b[0;32m    250\u001b[0m ]\n",
      "\u001b[1;31mValueError\u001b[0m: Missing column provided to 'parse_dates': 'Order_Date'"
     ]
    }
   ],
   "source": [
    "# Read CSV and change date column from object to date type\n",
    "sales_Apr = pd.read_csv(\"data\\Sales_April_2019.csv\", parse_dates =['Order_Date'])\n",
    "sales_Feb = pd.read_csv(\"data\\Sales_February_2019.csv\", parse_dates =['Order_Date'])\n",
    "sales_Jan = pd.read_csv(\"data\\Sales_January_2019.csv\",parse_dates=['Order_Date'])\n",
    "sales_Jun = pd.read_csv(\"data\\Sales_June_2019.csv\",parse_dates=['Order_Date'])\n",
    "sales_Mar = pd.read_csv(\"data\\Sales_March_2019.csv\",parse_dates=['Order_Date'])\n",
    "sales_May = pd.read_csv(\"data\\Sales_May_2019.csv\",parse_dates=['Order_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13622 entries, 0 to 13621\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Order_ID          13579 non-null  object\n",
      " 1   Product           13579 non-null  object\n",
      " 2   Quantity_Ordered  13579 non-null  object\n",
      " 3   Price_Each        13579 non-null  object\n",
      " 4   Order_Date        13579 non-null  object\n",
      " 5   Purchase_Address  13579 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 638.7+ KB\n"
     ]
    }
   ],
   "source": [
    "sales_Jun.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the names of the dataframes from Onedrive\n",
    "sales_Apr = sales_Apr.rename(columns={'Order ID': 'Order_ID', 'Quantity Ordered': 'Quantity_Ordered','Price Each':'Price_Each','Order Date':'Order_Date','Purchase Address':'Purchase_Address'})\n",
    "sales_Feb = sales_Feb.rename(columns={'Order ID': 'Order_ID', 'Quantity Ordered': 'Quantity_Ordered','Price Each':'Price_Each','Order Date':'Order_Date','Purchase Address':'Purchase_Address'})\n",
    "sales_Jan = sales_Jan.rename(columns={'Order ID': 'Order_ID', 'Quantity Ordered': 'Quantity_Ordered','Price Each':'Price_Each','Order Date':'Order_Date','Purchase Address':'Purchase_Address'})\n",
    "sales_Jun = sales_Jun.rename(columns={'Order ID': 'Order_ID', 'Quantity Ordered': 'Quantity_Ordered','Price Each':'Price_Each','Order Date':'Order_Date','Purchase Address':'Purchase_Address'})\n",
    "sales_Mar = sales_Mar.rename(columns={'Order ID': 'Order_ID', 'Quantity Ordered': 'Quantity_Ordered','Price Each':'Price_Each','Order Date':'Order_Date','Purchase Address':'Purchase_Address'})\n",
    "sales_May = sales_May.rename(columns={'Order ID': 'Order_ID', 'Quantity Ordered': 'Quantity_Ordered','Price Each':'Price_Each','Order Date':'Order_Date','Purchase Address':'Purchase_Address'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16635 entries, 0 to 16634\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Order_ID          16587 non-null  object\n",
      " 1   Product           16587 non-null  object\n",
      " 2   Quantity_Ordered  16587 non-null  object\n",
      " 3   Price_Each        16587 non-null  object\n",
      " 4   Order_Date        16587 non-null  object\n",
      " 5   Purchase_Address  16587 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 779.9+ KB\n"
     ]
    }
   ],
   "source": [
    "sales_May.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the data type of columns order_id, quantity_ordered, price_each\n",
    "sales_Apr[['Column1', 'Column2']] = df[['Column1', 'Column2']].astype(float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
